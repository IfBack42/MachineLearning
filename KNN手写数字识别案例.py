"""
KNNè¯†åˆ«æ‰‹å†™æ•°å­—ç°åº¦å›¾
æ¯å¼ å›¾éƒ½æ˜¯ç”± 28*28 åƒç´ ç»„æˆçš„ï¼Œè¡¨ç¤ºä¸º ä¸€è¡Œæ•°æ®æœ‰784ä¸ªåƒç´ ç‚¹ï¼Œæ¯ä¸ªåƒç´ ç‚¹çš„å€¼ä¸ºè¯¥åƒç´ çš„é¢œè‰²

"""
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV,train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score              # æ¨¡å‹è¯„ä¼°å‚æ•°
import matplotlib.pyplot as plt
import pandas as pd
import joblib  # ç”¨äºä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè®°å½•å‚æ•°
from collections import Counter


def data_load():
    px_pd = pd.read_csv("./data/æ‰‹å†™æ•°å­—è¯†åˆ«.csv")
    return px_pd

# å®šä¹‰å›¾åƒæ˜¾ç¤ºå‡½æ•°ï¼ŒæŠŠåƒç´ æ•°å€¼æ˜¾ç¤ºä¸ºç°åº¦å›¾
def pic_visual(px_pd,idx):
    y = px_pd.iloc[:, 0]
    x = px_pd.iloc[:, 1:]
    print(f"æ‰€æœ‰æ ‡ç­¾åˆ†å¸ƒæƒ…å†µï¼š{y.value_counts()}")   # ä¸ºserieså¯¹è±¡ï¼Œä½¿ç”¨æ•ˆæœåŒCounterï¼ˆyï¼‰ï¼ˆå­—å…¸ï¼‰ï¼Œä¸ç”¨å¯¼æ ¼å¤–çš„åŒ…
    print(f"{idx}ç´¢å¼•å¯¹åº”æ•°å­—æ˜¯ï¼š{y.iloc[idx]}")                  # ç´¢å¼•å¯¹åº”æ•°å­—
    print(f"{idx}ç´¢å¼•æ‰€åœ¨è¡Œå½¢çŠ¶ä¸ºï¼š{x.iloc[idx,:].shape}")        # ç´¢å¼•æ‰€åœ¨è¡Œå½¢çŠ¶ä¸ºï¼š(784,)
    reshaped_x = x.iloc[idx,:].values.reshape(28,28)           # é‡å¡‘å‹åä¸ºï¼š(28, 28)
    print(f"é‡å¡‘å‹åä¸ºï¼š{reshaped_x.shape}")
    plt.imshow(reshaped_x,cmap='gray')   # cmap -> å°†æ•°å€¼è½¬åŒ–ä¸ºçš„å›¾åƒç±»åˆ«ï¼Œâ€˜grayâ€™å±•ç¤ºç°åº¦å›¾
    plt.axis('off')                      # å…³é—­æ˜¾ç¤ºç°åº¦å›¾çš„åæ ‡
    plt.show()

def model_train(px_pd):
    y = px_pd.iloc[:, 0].to_numpy()  # to_numpy -> æ‹¿DFå’ŒSRå¯¹è±¡çš„æ•°ç»„
    x = px_pd.iloc[:, 1:].to_numpy()
    # å› ä¸ºæ•°æ®æ¯”è¾ƒå¹²å‡€æ‰€ä»¥ä½¿ç”¨å½’ä¸€åŒ–
    transfer = MinMaxScaler()
    x = transfer.fit_transform(x)
    # åˆ’åˆ†æ•°æ®é›† ğŸ‘‡åˆ’åˆ†é¡ºåºä¸èƒ½ä¹±,æŠ¥é”™å‡ æ¬¡äº†
    x_train,x_test,y_train,y_test = train_test_split(x,y,train_size=0.8,stratify=y) # stratifyå‚è€ƒæ ‡ç­¾è¿›è¡ŒæŠ½å–ï¼Œé˜²æ­¢æ•°æ®é›†æç«¯åˆ†å¸ƒ
    # åˆ›å»ºæ¨¡å‹ï¼Œä¸¢ç»™ç½‘æ ¼æœç´¢å’Œäº¤å‰éªŒè¯
    estimator = KNeighborsClassifier(n_neighbors=3)
    # # å®šä¹‰å¯èƒ½çš„å‚æ•°ç»„åˆ
    # param_dict = {'n_neighbors':[i for i in range(1,11)]}
    # # å®šä¹‰ç½‘æ ¼æœç´¢äº¤å‰éªŒè¯æ¨¡å‹
    # search_model = GridSearchCV(estimator=estimator,param_grid=param_dict,cv=4)
    # # è®­ç»ƒæ¨¡å‹ï¼Œæ‰¾åˆ°æœ€ä½³å‚æ•°
    # search_model.fit(x_train,y_train)
    # # æ‹¿åˆ°æœ€ä½³å‚æ•°å’Œæ¨¡å‹
    # print(f"æœ€ä¼˜è¯„åˆ†ï¼š{search_model.best_score_}")
    # print(f"æœ€ä¼˜è¶…å‚ç»„åˆï¼š{search_model.best_params_}")
    # print(f"æœ€ä¼˜æ¨¡å‹å¯¹è±¡ï¼š{search_model.best_estimator_}")
    # print(f"å…·ä½“äº¤å‰éªŒè¯ç»“æœï¼š{search_model.cv_results_}")
    # estimator = search_model.best_estimator_
    # æœ€åè®­ç»ƒæœ€ä¼˜æ¨¡å‹
    estimator.fit(x_train,y_train)
    result = estimator.predict(x_test)
    # æ¨¡å‹è¯„åˆ†
    print(f"æ¨¡å‹å‡†ç¡®ç‡ï¼š{accuracy_score(y_test,result)}")
    print(f"æ¨¡å‹å‡†ç¡®ç‡ï¼š{estimator.score(x_test,y_test)}")
    # ä¿å­˜æ¨¡å‹
    joblib.dump(estimator,"./æ‰‹å†™æ•°å­—è¯†åˆ«æ¨¡å‹.pkl") #pkl -> pickleæ–‡ä»¶

def use_model():  # testå¼€å¤´çš„å‡½æ•°ä¼šè°ƒç”¨ä¸“é—¨çš„åŒ…
    # åŠ è½½æ•°æ®
    im_data = plt.imread('./data0/demo.png') # å…¶å®å°±æ˜¯ç‰¹å¾äºŒç»´æ•°æ®, x
    print(im_data.shape)
    x = im_data.reshape(1,-1) # -1 è¡¨ç¤ºèƒ½è½¬ä¸ºå¤šå°‘åˆ—è½¬ä¸ºå¤šå°‘åˆ—
    print(x) # â­è¿™é‡Œçš„å›¾ç‰‡åƒç´ èŒƒå›´æ˜¯(0,1) è€Œä¸æ˜¯csvæ•°æ®é›†çš„ (0,255)
    # ä¸è¦ è¿›è¡Œæ•°æ®å½’ä¸€åŒ–
    # transfer = MinMaxScaler()
    # x = transfer.fit_transform(x)
    ## æ˜¾ç¤ºå›¾ç‰‡
    # plt.imshow(im_data,cmap='gray')
    # plt.axis('off')
    # plt.show()
    # åŠ è½½æ¨¡å‹
    estimator = joblib.load('./æ‰‹å†™æ•°å­—è¯†åˆ«æ¨¡å‹.pkl')
    # æ¨¡å‹é¢„æµ‹
    pre_result = estimator.predict(x)
    print(pre_result)



if __name__ == '__main__':
    px_pd = data_load()
    # idx = int(input("?"))
    # pic_visual(px_pd,idx)
    # model_train(px_pd)
    use_model()